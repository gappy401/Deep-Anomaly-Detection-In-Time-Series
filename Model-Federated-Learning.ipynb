{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# data Preprocessing\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data.dropna()  \n",
    "    # convert timestamp to datetime\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    return data\n",
    "\n",
    "#  the dataset\n",
    "file_path = 'Metro-Both-Classes.csv'\n",
    "data = load_and_preprocess_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>TP2</th>\n",
       "      <th>TP3</th>\n",
       "      <th>H1</th>\n",
       "      <th>DV_pressure</th>\n",
       "      <th>Reservoirs</th>\n",
       "      <th>Oil_temperature</th>\n",
       "      <th>Motor_current</th>\n",
       "      <th>COMP</th>\n",
       "      <th>DV_eletric</th>\n",
       "      <th>Towers</th>\n",
       "      <th>MPG</th>\n",
       "      <th>LPS</th>\n",
       "      <th>Pressure_switch</th>\n",
       "      <th>Oil_level</th>\n",
       "      <th>Caudal_impulses</th>\n",
       "      <th>class</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-01 00:00:09</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>8.594</td>\n",
       "      <td>8.582</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>8.598</td>\n",
       "      <td>59.000</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-01 00:00:18</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>8.580</td>\n",
       "      <td>8.568</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>8.584</td>\n",
       "      <td>58.950</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-01 00:00:28</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>8.566</td>\n",
       "      <td>8.554</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>8.570</td>\n",
       "      <td>58.800</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-01 00:00:38</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>8.552</td>\n",
       "      <td>8.540</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>8.556</td>\n",
       "      <td>58.700</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-01 00:00:48</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>8.536</td>\n",
       "      <td>8.526</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>8.540</td>\n",
       "      <td>58.575</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp    TP2    TP3     H1  DV_pressure  Reservoirs  \\\n",
       "0 2020-04-01 00:00:09 -0.014  8.594  8.582       -0.024       8.598   \n",
       "1 2020-04-01 00:00:18 -0.014  8.580  8.568       -0.024       8.584   \n",
       "2 2020-04-01 00:00:28 -0.014  8.566  8.554       -0.024       8.570   \n",
       "3 2020-04-01 00:00:38 -0.014  8.552  8.540       -0.022       8.556   \n",
       "4 2020-04-01 00:00:48 -0.014  8.536  8.526       -0.024       8.540   \n",
       "\n",
       "   Oil_temperature  Motor_current  COMP  DV_eletric  Towers  MPG  LPS  \\\n",
       "0           59.000         0.0450   1.0         0.0     1.0  1.0  0.0   \n",
       "1           58.950         0.0425   1.0         0.0     1.0  1.0  0.0   \n",
       "2           58.800         0.0425   1.0         0.0     1.0  1.0  0.0   \n",
       "3           58.700         0.0425   1.0         0.0     1.0  1.0  0.0   \n",
       "4           58.575         0.0425   1.0         0.0     1.0  1.0  0.0   \n",
       "\n",
       "   Pressure_switch  Oil_level  Caudal_impulses  class    month  \n",
       "0              1.0        1.0              1.0      0  2020-04  \n",
       "1              1.0        1.0              1.0      0  2020-04  \n",
       "2              1.0        1.0              1.0      0  2020-04  \n",
       "3              1.0        1.0              1.0      0  2020-04  \n",
       "4              1.0        1.0              1.0      0  2020-04  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def prepare_features_and_labels(data):\n",
    "    features = ['TP2', 'DV_pressure', 'Oil_temperature', 'Motor_current', 'DV_eletric', 'Towers', 'LPS', 'Oil_level', 'Caudal_impulses']\n",
    "    target = 'class'\n",
    "\n",
    "    # Select features for X and target for y\n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "    return X, y\n",
    "\n",
    "def scale_features(X_train, X_test=None):\n",
    "    \"\"\"Scale features using StandardScaler.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    if X_test is not None:\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        return X_train_scaled, X_test_scaled\n",
    "    \n",
    "    return X_train_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def train_global_model(X_global, y_global):\n",
    "    \"\"\"Train initial global SVM model.\"\"\"\n",
    "    model = SVC(kernel='rbf')\n",
    "    model.fit(X_global, y_global)\n",
    "    return model\n",
    "'''\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "def train_global_model(X_global, y_global):\n",
    "    \"\"\"Train initial global SVM model using a subset of the data, balance the classes, and avoid overfitting.\"\"\"\n",
    "    \n",
    "    # Shuffle and split data\n",
    "    X, y = shuffle(X_global, y_global, random_state=42)\n",
    "    X_train, X_sample, y_train, y_sample = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "    \n",
    "    # Balance classes using class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_sample), y=y_sample)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    \n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_balanced = scaler.fit_transform(X_sample)\n",
    "    \n",
    "    # Hyperparameter tuning with StratifiedKFold\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100, 1000],\n",
    "        'gamma': [0.001, 0.01, 0.1, 1],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(SVC(class_weight=class_weight_dict), param_grid, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "    grid_search.fit(X_balanced, y_sample)\n",
    "    \n",
    "    # Best model from grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "\n",
    "def local_training_and_update(model, X_local, y_local):\n",
    "    \"\"\"Fine-tune the global model on local data.\"\"\"\n",
    "    model.fit(X_local, y_local)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Model SVM Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"Model SVM Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def federated_learning(global_data, client1_data, client2_data):\n",
    "    \"\"\"Perform federated learning with LSTM-CNN and track progress using tqdm.\"\"\"\n",
    "    \n",
    "    # Prepare global data\n",
    "    print(\"Preparing global data...\")\n",
    "    X_global, y_global = prepare_features_and_labels(global_data)\n",
    "    X_global_scaled = scale_features(X_global)\n",
    "    \n",
    "    # Train global model\n",
    "    print(\"Training global model...\")\n",
    "    global_model = train_global_model(X_global_scaled, y_global)\n",
    "    \n",
    "    # Prepare client data\n",
    "    print(\"Preparing client data...\")\n",
    "    X_client1, y_client1 = prepare_features_and_labels(client1_data)\n",
    "    X_client2, y_client2 = prepare_features_and_labels(client2_data)\n",
    "    X_client1_scaled, X_client2_scaled = scale_features(X_client1, X_client2)\n",
    "    \n",
    "    # Fine-tune model on local client data\n",
    "    print(\"Fine-tuning model on client data...\")\n",
    "    \n",
    "    client_data = [\n",
    "        (X_client1_scaled, y_client1, \"Client 1\"),\n",
    "        (X_client2_scaled, y_client2, \"Client 2\")\n",
    "    ]\n",
    "    \n",
    "    for X_client, y_client, client_name in tqdm(client_data, desc=\"Clients\", unit=\"client\"):\n",
    "        print(f\"Fine-tuning on {client_name}...\")\n",
    "        start_time = time.time()\n",
    "        global_model = local_training_and_update(global_model, X_client, y_client)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Finished fine-tuning on {client_name} in {elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    # Evaluate the updated global model\n",
    "    print(\"Evaluating the updated global model...\")\n",
    "    start_time = time.time()\n",
    "    evaluate_model(global_model, X_global_scaled, y_global)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Model evaluation completed in {elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    print(\"Federated learning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def preprocess_data(data):\n",
    "   \n",
    "    if 'timestamp' not in data.columns or 'class' not in data.columns:\n",
    "        raise ValueError(\"Data must contain 'timestamp' and 'class' columns.\")\n",
    "    \n",
    "    # Ensure 'timestamp' is in datetime format\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with invalid timestamps\n",
    "    data = data.dropna(subset=['timestamp'])\n",
    "    \n",
    "    # Extract month as a period\n",
    "    data['month'] = data['timestamp'].dt.to_period('M')\n",
    "    \n",
    "    # Convert periods to string for sorting\n",
    "    months = data['month'].astype(str).unique()\n",
    "    \n",
    "    if len(months) < 4:\n",
    "        raise ValueError(\"Not enough distinct months to split into global and client data.\")\n",
    "    \n",
    "    months.sort()\n",
    "    print(months)\n",
    "    \n",
    "    first_two_months = months[:2]\n",
    "    last_two_months = months[-2:]\n",
    "    \n",
    "    # Filter data based on these months\n",
    "    global_data = data[data['month'].astype(str).isin(first_two_months)]\n",
    "    client1_data = data[data['month'].astype(str) == last_two_months[0]]\n",
    "    client2_data = data[data['month'].astype(str) == last_two_months[1]]\n",
    "    \n",
    "    if global_data.empty or client1_data.empty or client2_data.empty:\n",
    "        raise ValueError(\"One or more of the filtered datasets are empty.\")\n",
    "    \n",
    "    return global_data, client1_data, client2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-04' '2020-05' '2020-06' '2020-07']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])  # Ensure timestamp is in datetime format\n",
    "global_data, client1_data, client2_data = preprocess_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing global data...\n",
      "Training global model...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "federated_learning(global_data, client1_data, client2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final model on federated learning with attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Data Preprocessing\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data.dropna()  \n",
    "    return data\n",
    "\n",
    "def prepare_features_and_labels(data):\n",
    "    features = ['TP2', 'DV_pressure', 'Oil_temperature', 'Motor_current', 'DV_eletric', 'Towers', 'LPS', 'Oil_level', 'Caudal_impulses']\n",
    "    target = 'class'\n",
    "    X = data[features].values\n",
    "    y = data[target].values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def balance_and_sample(X, y, sample_fraction=0.4):\n",
    "    \"\"\"Sample and balance classes.\"\"\"\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    sample_size = int(sample_fraction * len(X))\n",
    "    X_sample, y_sample = X[:sample_size], y[:sample_size]\n",
    "    \n",
    "    classes = np.unique(y_sample)\n",
    "    max_samples = max([np.sum(y_sample == cls) for cls in classes])\n",
    "    \n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        X_cls = X_sample[y_sample == cls]\n",
    "        y_cls = y_sample[y_sample == cls]\n",
    "        \n",
    "        X_balanced.append(X_cls[:max_samples])\n",
    "        y_balanced.append(y_cls[:max_samples])\n",
    "    \n",
    "    X_balanced = np.vstack(X_balanced)\n",
    "    y_balanced = np.hstack(y_balanced)\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    if 'timestamp' not in data.columns or 'class' not in data.columns:\n",
    "        raise ValueError(\"Data must contain 'timestamp' and 'class' columns.\")\n",
    "    \n",
    "    # Ensure 'timestamp' is in datetime format\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with invalid timestamps\n",
    "    data = data.dropna(subset=['timestamp'])\n",
    "    \n",
    "    # Extract month as a period\n",
    "    data['month'] = data['timestamp'].dt.to_period('M')\n",
    "    \n",
    "    # Convert periods to string for sorting\n",
    "    months = data['month'].astype(str).unique()\n",
    "    \n",
    "    if len(months) < 4:\n",
    "        raise ValueError(\"Not enough distinct months to split into global and client data.\")\n",
    "    \n",
    "    months.sort()\n",
    "    \n",
    "    first_two_months = months[:2]\n",
    "    last_two_months = months[-2:]\n",
    "    \n",
    "    # Filter data based on these months\n",
    "    global_data = data[data['month'].astype(str).isin(first_two_months)]\n",
    "    client1_data = data[data['month'].astype(str) == last_two_months[0]]\n",
    "    client2_data = data[data['month'].astype(str) == last_two_months[1]]\n",
    "    \n",
    "    if global_data.empty or client1_data.empty or client2_data.empty:\n",
    "        raise ValueError(\"One or more of the filtered datasets are empty.\")\n",
    "    \n",
    "    return global_data, client1_data, client2_data\n",
    "\n",
    "def federated_learning(global_data, client1_data, client2_data):\n",
    "    \"\"\"Perform federated learning with CNN-LSTM and track progress using tqdm.\"\"\"\n",
    "    \n",
    "    # Prepare global data\n",
    "    print(\"Preparing global data...\")\n",
    "    X_global, y_global = prepare_features_and_labels(global_data)\n",
    "    X_global, y_global = balance_and_sample(X_global, y_global, sample_fraction=0.4)\n",
    "    \n",
    "    # Prepare client data\n",
    "    print(\"Preparing client data...\")\n",
    "    X_client1, y_client1 = prepare_features_and_labels(client1_data)\n",
    "    X_client2, y_client2 = prepare_features_and_labels(client2_data)\n",
    "    X_client1, y_client1 = balance_and_sample(X_client1, y_client1, sample_fraction=0.4)\n",
    "    X_client2, y_client2 = balance_and_sample(X_client2, y_client2, sample_fraction=0.4)\n",
    "    \n",
    "    # Reshape data for CNN-LSTM (e.g., [samples, timesteps, features])\n",
    "    # Assuming 1 timestep and number of features as the second dimension\n",
    "    X_global = X_global[:, np.newaxis, :]\n",
    "    X_client1 = X_client1[:, np.newaxis, :]\n",
    "    X_client2 = X_client2[:, np.newaxis, :]\n",
    "    \n",
    "    # Create and train global model\n",
    "    print(\"Creating and training global model...\")\n",
    "    model = create_cnn_lstm_model(input_shape=(X_global.shape[1], X_global.shape[2]))\n",
    "    model.fit(X_global, y_global, epochs=10, batch_size=32, verbose=2)\n",
    "    \n",
    "    # Fine-tune model on client data\n",
    "    client_data = [\n",
    "        (X_client1, y_client1, \"Client 1\"),\n",
    "        (X_client2, y_client2, \"Client 2\")\n",
    "    ]\n",
    "    \n",
    "    print(\"Fine-tuning model on client data...\")\n",
    "    for X_client, y_client, client_name in tqdm(client_data, desc=\"Clients\", unit=\"client\"):\n",
    "        print(f\"Fine-tuning on {client_name}...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(X_client, y_client, epochs=5, batch_size=32, verbose=2)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Finished fine-tuning on {client_name} in {elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    # Evaluate the updated global model\n",
    "    print(\"Evaluating the updated global model...\")\n",
    "    start_time = time.time()\n",
    "    y_global_pred = (model.predict(X_global) > 0.5).astype(int)\n",
    "    print(\"Model Classification Report:\")\n",
    "    print(classification_report(y_global, y_global_pred))\n",
    "    print(\"Model Accuracy Score:\", accuracy_score(y_global, y_global_pred))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Model evaluation completed in {elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    print(\"Federated learning completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(input_shape):\n",
    "    \"\"\"Create a CNN-LSTM model.\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN layers with padding to handle small sequence lengths\n",
    "    x = Conv1D(filters=64, kernel_size=1, activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling1D(pool_size=1)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # LSTM layers\n",
    "    x = LSTM(50, return_sequences=True)(x)\n",
    "    x = LSTM(50)(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing global data...\n",
      "Preparing client data...\n",
      "Creating and training global model...\n",
      "Epoch 1/10\n",
      "5145/5145 - 18s - 3ms/step - accuracy: 0.9731 - loss: 0.0613\n",
      "Epoch 2/10\n",
      "5145/5145 - 13s - 3ms/step - accuracy: 0.9762 - loss: 0.0451\n",
      "Epoch 3/10\n",
      "5145/5145 - 13s - 3ms/step - accuracy: 0.9875 - loss: 0.0306\n",
      "Epoch 4/10\n",
      "5145/5145 - 13s - 2ms/step - accuracy: 0.9900 - loss: 0.0259\n",
      "Epoch 5/10\n",
      "5145/5145 - 13s - 3ms/step - accuracy: 0.9920 - loss: 0.0230\n",
      "Epoch 6/10\n",
      "5145/5145 - 13s - 3ms/step - accuracy: 0.9920 - loss: 0.0223\n",
      "Epoch 7/10\n",
      "5145/5145 - 13s - 3ms/step - accuracy: 0.9921 - loss: 0.0224\n",
      "Epoch 8/10\n",
      "5145/5145 - 13s - 3ms/step - accuracy: 0.9926 - loss: 0.0215\n",
      "Epoch 9/10\n",
      "5145/5145 - 13s - 3ms/step - accuracy: 0.9928 - loss: 0.0211\n",
      "Epoch 10/10\n",
      "5145/5145 - 13s - 3ms/step - accuracy: 0.9924 - loss: 0.0217\n",
      "Fine-tuning model on client data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clients:   0%|          | 0/2 [00:00<?, ?client/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning on Client 1...\n",
      "Epoch 1/5\n",
      "2707/2707 - 7s - 3ms/step - accuracy: 0.9969 - loss: 0.0107\n",
      "Epoch 2/5\n",
      "2707/2707 - 7s - 3ms/step - accuracy: 0.9974 - loss: 0.0084\n",
      "Epoch 3/5\n",
      "2707/2707 - 7s - 3ms/step - accuracy: 0.9979 - loss: 0.0072\n",
      "Epoch 4/5\n",
      "2707/2707 - 8s - 3ms/step - accuracy: 0.9982 - loss: 0.0071\n",
      "Epoch 5/5\n",
      "2707/2707 - 7s - 3ms/step - accuracy: 0.9979 - loss: 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clients:  50%|█████     | 1/2 [00:35<00:35, 35.97s/client]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fine-tuning on Client 1 in 35.97 seconds.\n",
      "Fine-tuning on Client 2...\n",
      "Epoch 1/5\n",
      "2783/2783 - 7s - 3ms/step - accuracy: 0.9938 - loss: 0.0163\n",
      "Epoch 2/5\n",
      "2783/2783 - 7s - 3ms/step - accuracy: 0.9957 - loss: 0.0121\n",
      "Epoch 3/5\n",
      "2783/2783 - 7s - 3ms/step - accuracy: 0.9964 - loss: 0.0108\n",
      "Epoch 4/5\n",
      "2783/2783 - 7s - 3ms/step - accuracy: 0.9969 - loss: 0.0098\n",
      "Epoch 5/5\n",
      "2783/2783 - 7s - 3ms/step - accuracy: 0.9972 - loss: 0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clients: 100%|██████████| 2/2 [01:12<00:00, 36.12s/client]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fine-tuning on Client 2 in 36.26 seconds.\n",
      "Evaluating the updated global model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5145/5145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step\n",
      "Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99    160227\n",
      "           1       0.00      0.00      0.00      4386\n",
      "\n",
      "    accuracy                           0.97    164613\n",
      "   macro avg       0.49      0.50      0.49    164613\n",
      "weighted avg       0.95      0.97      0.96    164613\n",
      "\n",
      "Model Accuracy Score: 0.9733435390886503\n",
      "Model evaluation completed in 10.68 seconds.\n",
      "Federated learning completed.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = 'Metro-Both-Classes.csv'\n",
    "data = load_and_preprocess_data(file_path)\n",
    "global_data, client1_data, client2_data = preprocess_data(data)\n",
    "federated_learning(global_data, client1_data, client2_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model with attention mechnism and gradient compression in a CNN LSTM w Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Attention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Data Preprocessing\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data.dropna()  \n",
    "    return data\n",
    "\n",
    "def prepare_features_and_labels(data):\n",
    "    features = ['TP2', 'DV_pressure', 'Oil_temperature', 'Motor_current', 'DV_eletric', 'Towers', 'LPS', 'Oil_level', 'Caudal_impulses']\n",
    "    target = 'class'\n",
    "    X = data[features].values\n",
    "    y = data[target].values\n",
    "    return X, y\n",
    "\n",
    "def balance_and_sample(X, y, sample_fraction=0.4):\n",
    "    \"\"\"Sample and balance classes.\"\"\"\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    sample_size = int(sample_fraction * len(X))\n",
    "    X_sample, y_sample = X[:sample_size], y[:sample_size]\n",
    "    \n",
    "    classes = np.unique(y_sample)\n",
    "    max_samples = max([np.sum(y_sample == cls) for cls in classes])\n",
    "    \n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        X_cls = X_sample[y_sample == cls]\n",
    "        y_cls = y_sample[y_sample == cls]\n",
    "        \n",
    "        X_balanced.append(X_cls[:max_samples])\n",
    "        y_balanced.append(y_cls[:max_samples])\n",
    "    \n",
    "    X_balanced = np.vstack(X_balanced)\n",
    "    y_balanced = np.hstack(y_balanced)\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "def preprocess_data(data):\n",
    "    if 'timestamp' not in data.columns or 'class' not in data.columns:\n",
    "        raise ValueError(\"Data must contain 'timestamp' and 'class' columns.\")\n",
    "    \n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')\n",
    "    data = data.dropna(subset=['timestamp'])\n",
    "    data['month'] = data['timestamp'].dt.to_period('M')\n",
    "    months = data['month'].astype(str).unique()\n",
    "    \n",
    "    if len(months) < 4:\n",
    "        raise ValueError(\"Not enough distinct months to split into global and client data.\")\n",
    "    \n",
    "    months.sort()\n",
    "    first_two_months = months[:2]\n",
    "    last_two_months = months[-2:]\n",
    "    \n",
    "    global_data = data[data['month'].astype(str).isin(first_two_months)]\n",
    "    client1_data = data[data['month'].astype(str) == last_two_months[0]]\n",
    "    client2_data = data[data['month'].astype(str) == last_two_months[1]]\n",
    "    \n",
    "    if global_data.empty or client1_data.empty or client2_data.empty:\n",
    "        raise ValueError(\"One or more of the filtered datasets are empty.\")\n",
    "    \n",
    "    return global_data, client1_data, client2_data\n",
    "\n",
    "\n",
    "\n",
    "def compress_gradients(gradients, compression_factor=0.1):\n",
    "    \"\"\"Quantize gradients to reduce communication overhead.\"\"\"\n",
    "    compressed_gradients = {}\n",
    "    for name, grad in gradients.items():\n",
    "        # Quantize gradients (e.g., to 8-bit precision)\n",
    "        compressed_gradients[name] = (grad / np.max(np.abs(grad)) * 127).astype(np.int8)\n",
    "    return compressed_gradients\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Multiply, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_cnn_lstm_attention_model(input_shape):\n",
    "    \"\"\"Create a CNN-LSTM model with attention mechanism.\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN layers\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # LSTM layers\n",
    "    x = LSTM(50, return_sequences=True)(x)\n",
    "    \n",
    "    # Attention Mechanism\n",
    "    attention_probs = Dense(x.shape[-1], activation='softmax')(x)  # Use the last dimension of x\n",
    "    attention_mul = Multiply()([x, attention_probs])\n",
    "    \n",
    "    x = LSTM(50)(attention_mul)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def measure_communication_time(func, *args, **kwargs):\n",
    "    \"\"\"Measure the time taken for a function to execute.\"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return result, elapsed_time\n",
    "\n",
    "def compress_gradients(gradients):\n",
    "    \"\"\"Apply gradient compression (e.g., quantization or sparsification).\"\"\"\n",
    "    compressed_gradients = []\n",
    "    for grad in gradients:\n",
    "        # Example of simple quantization: round gradients to the nearest integer\n",
    "        compressed_gradients.append(tf.round(grad))\n",
    "    return compressed_gradients\n",
    "\n",
    "def federated_learning(global_data, client1_data, client2_data):\n",
    "    \"\"\"Perform federated learning with CNN-LSTM and attention mechanism, tracking communication time.\"\"\"\n",
    "    \n",
    "    # Prepare global data\n",
    "    print(\"Preparing global data...\")\n",
    "    X_global, y_global = prepare_features_and_labels(global_data)\n",
    "    X_global, y_global = balance_and_sample(X_global, y_global, sample_fraction=0.4)\n",
    "    \n",
    "    # Prepare client data\n",
    "    print(\"Preparing client data...\")\n",
    "    X_client1, y_client1 = prepare_features_and_labels(client1_data)\n",
    "    X_client2, y_client2 = prepare_features_and_labels(client2_data)\n",
    "    X_client1, y_client1 = balance_and_sample(X_client1, y_client1, sample_fraction=0.4)\n",
    "    X_client2, y_client2 = balance_and_sample(X_client2, y_client2, sample_fraction=0.4)\n",
    "    \n",
    "    # Reshape data for CNN-LSTM (e.g., [samples, timesteps, features])\n",
    "    X_global = X_global[:, np.newaxis, :]\n",
    "    X_client1 = X_client1[:, np.newaxis, :]\n",
    "    X_client2 = X_client2[:, np.newaxis, :]\n",
    "    \n",
    "    # Create and train global model\n",
    "    print(\"Creating and training global model...\")\n",
    "    model = create_cnn_lstm_attention_model(input_shape=(X_global.shape[1], X_global.shape[2]))\n",
    "    _, train_time_global = measure_communication_time(model.fit, X_global, y_global, epochs=10, batch_size=32, verbose=2)\n",
    "    print(f\"Time to train global model: {train_time_global:.2f} seconds.\")\n",
    "    \n",
    "    # Fine-tune model on client data\n",
    "    client_data = [\n",
    "        (X_client1, y_client1, \"Client 1\"),\n",
    "        (X_client2, y_client2, \"Client 2\")\n",
    "    ]\n",
    "    \n",
    "    print(\"Fine-tuning model on client data...\")\n",
    "    for X_client, y_client, client_name in tqdm(client_data, desc=\"Clients\", unit=\"client\"):\n",
    "        print(f\"Fine-tuning on {client_name}...\")\n",
    "        \n",
    "        # Measure time to fine-tune\n",
    "        def train_func():\n",
    "            model.fit(X_client, y_client, epochs=5, batch_size=32, verbose=2)\n",
    "        \n",
    "        _, update_time = measure_communication_time(train_func)\n",
    "        print(f\"Time to fine-tune on {client_name}: {update_time:.2f} seconds.\")\n",
    "        \n",
    "        # Compress gradients and simulate communication\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Calculate logits\n",
    "            logits = model(X_client, training=True)\n",
    "            \n",
    "            # Ensure y_client has the correct shape\n",
    "            y_client = tf.reshape(y_client, (-1, 1))\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = tf.keras.losses.binary_crossentropy(y_client, logits)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        compressed_gradients = compress_gradients(gradients)\n",
    "        print(f\"Compressed gradients size for {client_name}: {sum(tf.size(grad).numpy() for grad in compressed_gradients) / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    # Evaluate the updated global model\n",
    "    print(\"Evaluating the updated global model...\")\n",
    "    y_global_pred = (model.predict(X_global) > 0.5).astype(int)\n",
    "    print(\"Model Classification Report:\")\n",
    "    print(classification_report(y_global, y_global_pred))\n",
    "    print(\"Model Accuracy Score:\", accuracy_score(y_global, y_global_pred))\n",
    "    \n",
    "    print(\"Federated learning completed.\")\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, Attention, Concatenate, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def create_advanced_cnn_lstm_attention_model(input_shape):\n",
    "    \"\"\"Create an advanced CNN-LSTM model with an enhanced attention mechanism.\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN layers\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # LSTM layers\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True))(x)\n",
    "    \n",
    "    # Attention Mechanism\n",
    "    attention_probs = Dense(x.shape[-1], activation='softmax')(x)  # Use the last dimension of x\n",
    "    attention_mul = Multiply()([x, attention_probs])\n",
    "    \n",
    "    # Further LSTM layer\n",
    "    x = Bidirectional(LSTM(100))(attention_mul)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing global data...\n",
      "Preparing client data...\n",
      "Creating and training global model...\n",
      "Epoch 1/10\n",
      "5145/5145 - 28s - 5ms/step - accuracy: 0.9733 - loss: 0.0661\n",
      "Epoch 2/10\n",
      "5145/5145 - 24s - 5ms/step - accuracy: 0.9734 - loss: 0.0487\n",
      "Epoch 3/10\n",
      "5145/5145 - 19s - 4ms/step - accuracy: 0.9819 - loss: 0.0382\n",
      "Epoch 4/10\n",
      "5145/5145 - 20s - 4ms/step - accuracy: 0.9895 - loss: 0.0277\n",
      "Epoch 5/10\n",
      "5145/5145 - 22s - 4ms/step - accuracy: 0.9909 - loss: 0.0246\n",
      "Epoch 6/10\n",
      "5145/5145 - 20s - 4ms/step - accuracy: 0.9915 - loss: 0.0231\n",
      "Epoch 7/10\n",
      "5145/5145 - 20s - 4ms/step - accuracy: 0.9917 - loss: 0.0230\n",
      "Epoch 8/10\n",
      "5145/5145 - 19s - 4ms/step - accuracy: 0.9921 - loss: 0.0225\n",
      "Epoch 9/10\n",
      "5145/5145 - 19s - 4ms/step - accuracy: 0.9930 - loss: 0.0205\n",
      "Epoch 10/10\n",
      "5145/5145 - 19s - 4ms/step - accuracy: 0.9931 - loss: 0.0202\n",
      "Time to train global model: 210.52 seconds.\n",
      "Fine-tuning model on client data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clients:   0%|          | 0/2 [00:00<?, ?client/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning on Client 1...\n",
      "Epoch 1/5\n",
      "2707/2707 - 9s - 3ms/step - accuracy: 0.9974 - loss: 0.0107\n",
      "Epoch 2/5\n",
      "2707/2707 - 9s - 3ms/step - accuracy: 0.9980 - loss: 0.0068\n",
      "Epoch 3/5\n",
      "2707/2707 - 9s - 3ms/step - accuracy: 0.9981 - loss: 0.0068\n",
      "Epoch 4/5\n",
      "2707/2707 - 9s - 3ms/step - accuracy: 0.9983 - loss: 0.0063\n",
      "Epoch 5/5\n",
      "2707/2707 - 13s - 5ms/step - accuracy: 0.9981 - loss: 0.0065\n",
      "Time to fine-tune on Client 1: 48.37 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clients:  50%|█████     | 1/2 [00:49<00:49, 49.69s/client]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed gradients size for Client 1: 0.05 MB\n",
      "Fine-tuning on Client 2...\n",
      "Epoch 1/5\n",
      "2783/2783 - 9s - 3ms/step - accuracy: 0.9934 - loss: 0.0191\n",
      "Epoch 2/5\n",
      "2783/2783 - 8s - 3ms/step - accuracy: 0.9964 - loss: 0.0113\n",
      "Epoch 3/5\n",
      "2783/2783 - 8s - 3ms/step - accuracy: 0.9974 - loss: 0.0081\n",
      "Epoch 4/5\n",
      "2783/2783 - 9s - 3ms/step - accuracy: 0.9974 - loss: 0.0081\n",
      "Epoch 5/5\n",
      "2783/2783 - 9s - 3ms/step - accuracy: 0.9976 - loss: 0.0085\n",
      "Time to fine-tune on Client 2: 43.20 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clients: 100%|██████████| 2/2 [01:33<00:00, 46.96s/client]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed gradients size for Client 2: 0.05 MB\n",
      "Evaluating the updated global model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5145/5145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step\n",
      "Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99    160227\n",
      "           1       0.00      0.00      0.00      4386\n",
      "\n",
      "    accuracy                           0.97    164613\n",
      "   macro avg       0.49      0.50      0.49    164613\n",
      "weighted avg       0.95      0.97      0.96    164613\n",
      "\n",
      "Model Accuracy Score: 0.9733556887973611\n",
      "Federated learning completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nandi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nandi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nandi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Metro-Both-Classes.csv'\n",
    "data = load_and_preprocess_data(file_path)\n",
    "global_data, client1_data, client2_data = preprocess_data(data)\n",
    "federated_learning(global_data, client1_data, client2_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
